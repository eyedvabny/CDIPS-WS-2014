{
 "metadata": {
  "name": "",
  "signature": "sha256:ceb633c7c29e9747fb0897f470f738460b1d2d6cada59fdb48705a8a38f21aad"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "import avito_modules as am"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Read the TSV file chunk by chunk to conserve memory. Adjust chunk size as needed. Need to read the TSV as it:\n",
      "1. can be chunked\n",
      "2. read by columns. Pickles are all-or-nothing :-(\n",
      "\n",
      "If you have enough mem to read the whole table and then store the cleaned word matrix, go for it"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "d_train = pd.read_csv(\"data/avito_train.tsv\",sep='\\t',chunksize=300000,usecols=[4],encoding=\"utf-8\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Split descriptions into words and make one long Series"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "word_set = pd.concat([chunk.description.apply(am.getWords,args=(False,False)) for chunk in d_train])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We need to get a list of unique words. For that all lists in the series should be joined and filtered."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "word_list = pd.Series([desc_word for desc_list in word_set for desc_word in desc_list]).unique()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So now we have a list of unique words in all the descriptions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(word_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "1030619"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Save this list so we don't need to reconstruct it again"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.save(\"data/train_desc_word_list_raw.npy\",word_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's also count the number of words in each ad"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "num_words = word_set.apply(len)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So now we have the words, the number of said words"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}